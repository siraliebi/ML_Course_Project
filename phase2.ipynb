{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd98cf7-119d-4b47-bc31-028f2332f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSVs to: artifacts/phase2\n",
      "Shapes: (923549, 62) (46830, 62) (46830, 62)\n",
      "Is 'Customers' leaked into features? False\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Phase 2 (Clean) â€” Feature Engineering + Save as CSV (Store NOT scaled)\n",
    "# =========================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "OUT_DIR = \"artifacts/phase2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "STORE_PATH = \"store.csv\"\n",
    "\n",
    "TRAIN_OUT = os.path.join(OUT_DIR, \"train_features.csv\")\n",
    "VAL_OUT   = os.path.join(OUT_DIR, \"val_features.csv\")\n",
    "TEST_OUT  = os.path.join(OUT_DIR, \"test_features.csv\")\n",
    "\n",
    "FEATURE_COLS_OUT = os.path.join(OUT_DIR, \"feature_cols.json\")\n",
    "SPLIT_META_OUT   = os.path.join(OUT_DIR, \"split_meta.json\")\n",
    "SCALER_OUT       = os.path.join(OUT_DIR, \"scaler.joblib\")\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load + Merge\n",
    "# ----------------------------\n",
    "train = pd.read_csv(TRAIN_PATH, low_memory=False) \n",
    "store = pd.read_csv(STORE_PATH)\n",
    "\n",
    "train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "if \"StateHoliday\" in train.columns:\n",
    "    train[\"StateHoliday\"] = train[\"StateHoliday\"].astype(str).str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "\n",
    "if \"Open\" in train.columns:\n",
    "    train[\"Open\"] = train[\"Open\"].fillna(1)\n",
    "\n",
    "df = train.merge(store, on=\"Store\", how=\"left\").sort_values([\"Store\",\"Date\"]).reset_index(drop=True)\n",
    "df[\"Store\"] = df[\"Store\"].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Feature Functions\n",
    "# ----------------------------\n",
    "def add_time_features(df):\n",
    "    df = df.copy()\n",
    "    d = df[\"Date\"]\n",
    "    df[\"year\"] = d.dt.year\n",
    "    df[\"month\"] = d.dt.month\n",
    "    df[\"day\"] = d.dt.day\n",
    "    df[\"weekofyear\"] = d.dt.isocalendar().week.astype(int)\n",
    "    df[\"dayofyear\"] = d.dt.dayofyear\n",
    "    df[\"is_weekend\"] = ((d.dt.dayofweek + 1) >= 6).astype(int)\n",
    "    if \"DayOfWeek\" in df.columns:\n",
    "        df[\"DayOfWeek\"] = df[\"DayOfWeek\"].astype(int)\n",
    "    df[\"DayOfWeek_calc\"] = d.dt.dayofweek + 1\n",
    "    df[\"is_month_start\"] = d.dt.is_month_start.astype(int)\n",
    "    df[\"is_month_end\"] = d.dt.is_month_end.astype(int)\n",
    "    return df\n",
    "\n",
    "def add_holiday_distance(df):\n",
    "   \n",
    "    df = df.copy()\n",
    "    sh = df[\"StateHoliday\"].astype(str) if \"StateHoliday\" in df.columns else \"0\"\n",
    "    school = df[\"SchoolHoliday\"].fillna(0).astype(int) if \"SchoolHoliday\" in df.columns else 0\n",
    "    df[\"is_holiday_any\"] = (((sh != \"0\") & (sh != \"nan\")) | (school == 1)).astype(int)\n",
    "\n",
    "    holiday_dates = np.sort(df.loc[df[\"is_holiday_any\"] == 1, \"Date\"].dropna().unique())\n",
    "    if len(holiday_dates) == 0:\n",
    "        df[\"days_to_next_holiday\"] = np.nan\n",
    "        df[\"days_since_prev_holiday\"] = np.nan\n",
    "        return df\n",
    "\n",
    "    dates = df[\"Date\"].values.astype(\"datetime64[D]\")\n",
    "    h = holiday_dates.astype(\"datetime64[D]\")\n",
    "\n",
    "    idx_next = np.searchsorted(h, dates, side=\"left\")\n",
    "    idx_prev = idx_next - 1\n",
    "\n",
    "    next_date = np.where(idx_next < len(h), h[idx_next], np.datetime64(\"NaT\"))\n",
    "    prev_date = np.where(idx_prev >= 0, h[idx_prev], np.datetime64(\"NaT\"))\n",
    "\n",
    "    df[\"days_to_next_holiday\"] = (next_date - dates).astype(\"timedelta64[D]\").astype(float)\n",
    "    df[\"days_since_prev_holiday\"] = (dates - prev_date).astype(\"timedelta64[D]\").astype(float)\n",
    "\n",
    "    df.loc[idx_next >= len(h), \"days_to_next_holiday\"] = np.nan\n",
    "    df.loc[idx_prev < 0, \"days_since_prev_holiday\"] = np.nan\n",
    "    return df\n",
    "\n",
    "def add_fourier_terms(df, period=365.25, order=3):\n",
    "    df = df.copy()\n",
    "    t = (df[\"Date\"] - df[\"Date\"].min()).dt.days.astype(float).values\n",
    "    for k in range(1, order + 1):\n",
    "        df[f\"fourier_sin_{k}\"] = np.sin(2 * np.pi * k * t / period)\n",
    "        df[f\"fourier_cos_{k}\"] = np.cos(2 * np.pi * k * t / period)\n",
    "    return df\n",
    "\n",
    "def add_store_derived(df):\n",
    "   \n",
    "    df = df.copy()\n",
    "    if \"CompetitionOpenSinceYear\" in df.columns and \"CompetitionOpenSinceMonth\" in df.columns:\n",
    "        comp_year = df[\"CompetitionOpenSinceYear\"]\n",
    "        comp_month = df[\"CompetitionOpenSinceMonth\"]\n",
    "        comp_date = pd.to_datetime(\n",
    "            dict(year=comp_year.fillna(1900).astype(int),\n",
    "                 month=comp_month.fillna(1).astype(int),\n",
    "                 day=1), errors=\"coerce\")\n",
    "        df[\"competition_age_days\"] = (df[\"Date\"] - comp_date).dt.days\n",
    "        df.loc[comp_year.isna() | comp_month.isna(), \"competition_age_days\"] = np.nan\n",
    "\n",
    "    if \"Promo2\" in df.columns and \"PromoInterval\" in df.columns:\n",
    "        promo_map = {\"Jan\":1,\"Feb\":2,\"Mar\":3,\"Apr\":4,\"May\":5,\"Jun\":6,\"Jul\":7,\"Aug\":8,\"Sept\":9,\"Oct\":10,\"Nov\":11,\"Dec\":12}\n",
    "        def interval_to_mask(s):\n",
    "            if pd.isna(s) or not isinstance(s, str) or not s.strip(): return 0\n",
    "            mask = 0\n",
    "            for p in [x.strip() for x in s.split(\",\")]:\n",
    "                m = promo_map.get(p)\n",
    "                if m: mask |= (1 << (m-1))\n",
    "            return mask\n",
    "\n",
    "        uniq = df[\"PromoInterval\"].fillna(\"\").unique()\n",
    "        mask_dict = {u: interval_to_mask(u) for u in uniq}\n",
    "        masks = df[\"PromoInterval\"].fillna(\"\").map(mask_dict).astype(int)\n",
    "\n",
    "        month_idx = (df[\"month\"].astype(int) - 1).to_numpy()\n",
    "        month_bit = np.left_shift(1, month_idx).astype(int)\n",
    "        masks_np = masks.to_numpy(dtype=int)\n",
    "\n",
    "        df[\"promo2_month_ok\"] = ((masks_np & month_bit) > 0).astype(int)\n",
    "        df[\"promo2_active\"] = ((df[\"Promo2\"].fillna(0).astype(int) == 1) & (df[\"promo2_month_ok\"] == 1)).astype(int)\n",
    "\n",
    "    if \"CompetitionDistance\" in df.columns:\n",
    "        df[\"has_competitor_distance\"] = df[\"CompetitionDistance\"].notna().astype(int)\n",
    "    return df\n",
    "\n",
    "def add_lag_rolling(df, lags=(1,7,14,28), windows=(7,14,28), minp_ratio=0.5):\n",
    "    df = df.copy().sort_values([\"Store\", \"Date\"])\n",
    "    g = df.groupby(\"Store\", sort=False)[\"Sales\"]\n",
    "\n",
    "    for L in lags:\n",
    "        df[f\"Sales_lag_{L}\"] = g.shift(L)\n",
    "\n",
    "    s = g.shift(1)  # no leakage\n",
    "    for w in windows:\n",
    "        minp = max(2, int(w * minp_ratio))\n",
    "        roll = s.groupby(df[\"Store\"]).rolling(window=w, min_periods=minp)\n",
    "        df[f\"Sales_roll_mean_{w}\"] = roll.mean().reset_index(level=0, drop=True)\n",
    "        df[f\"Sales_roll_std_{w}\"]  = roll.std().reset_index(level=0, drop=True)\n",
    "    return df\n",
    "\n",
    "def add_cum_means(df):\n",
    "    df = df.copy().sort_values([\"Store\",\"Date\"])\n",
    "    cs = df.groupby(\"Store\")[\"Sales\"].cumsum()\n",
    "    cnt = df.groupby(\"Store\").cumcount()\n",
    "    df[\"Sales_cum_mean_store\"] = (cs - df[\"Sales\"]) / cnt.replace(0, np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_categoricals(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    for c in [\"Open\",\"Promo\",\"SchoolHoliday\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(0)\n",
    "\n",
    "    cat_cols = [c for c in [\"StoreType\", \"Assortment\", \"StateHoliday\", \"PromoInterval\"] if c in df.columns]\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].fillna(\"NA\").astype(str)\n",
    "\n",
    "    if cat_cols:\n",
    "        df = pd.get_dummies(df, columns=cat_cols, drop_first=False)\n",
    "    return df\n",
    "\n",
    "def time_split(df, test_days=42, val_days=42):\n",
    "    max_date = df[\"Date\"].max()\n",
    "    test_start = max_date - pd.Timedelta(days=test_days - 1)\n",
    "    val_start  = test_start - pd.Timedelta(days=val_days)\n",
    "\n",
    "    train_df = df[df[\"Date\"] < val_start].copy()\n",
    "    val_df   = df[(df[\"Date\"] >= val_start) & (df[\"Date\"] < test_start)].copy()\n",
    "    test_df  = df[df[\"Date\"] >= test_start].copy()\n",
    "\n",
    "    meta = {\n",
    "        \"max_date\": str(max_date.date()),\n",
    "        \"val_start\": str(val_start.date()),\n",
    "        \"test_start\": str(test_start.date()),\n",
    "        \"train_rows\": int(len(train_df)),\n",
    "        \"val_rows\": int(len(val_df)),\n",
    "        \"test_rows\": int(len(test_df)),\n",
    "        \"test_days\": int(test_days),\n",
    "        \"val_days\": int(val_days),\n",
    "    }\n",
    "    return train_df, val_df, test_df, meta\n",
    "\n",
    "def impute_missing(train_df, val_df, test_df):\n",
    "    num_cols = train_df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    \n",
    "    for c in num_cols:\n",
    "        if c in [\"Sales\", \"Store\", \"Customers\"]:\n",
    "            continue\n",
    "        if train_df[c].isna().any() or val_df[c].isna().any() or test_df[c].isna().any():\n",
    "          \n",
    "            median_val = train_df[c].median()\n",
    "            train_df[c] = train_df[c].fillna(median_val)\n",
    "            val_df[c]   = val_df[c].fillna(median_val)\n",
    "            test_df[c]  = test_df[c].fillna(median_val)\n",
    "            \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Build features\n",
    "# ----------------------------\n",
    "df2 = df.copy()\n",
    "df2 = add_time_features(df2)\n",
    "df2 = add_holiday_distance(df2)\n",
    "df2 = add_fourier_terms(df2, order=3)\n",
    "df2 = add_store_derived(df2)\n",
    "\n",
    "df2 = add_lag_rolling(df2)\n",
    "df2 = add_cum_means(df2)\n",
    "\n",
    "if \"Promo\" in df2.columns and \"SchoolHoliday\" in df2.columns:\n",
    "    df2[\"promo_x_schoolholiday\"] = df2[\"Promo\"].astype(int) * df2[\"SchoolHoliday\"].astype(int)\n",
    "\n",
    "df2 = encode_categoricals(df2)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Split\n",
    "# ----------------------------\n",
    "train_df, val_df, test_df, split_meta = time_split(df2, test_days=42, val_days=42)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Handle Missing (Strictly AFTER split)\n",
    "# ----------------------------\n",
    "\n",
    "train_df, val_df, test_df = impute_missing(train_df, val_df, test_df)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Define features \n",
    "# ----------------------------\n",
    "TARGET = \"Sales\"\n",
    "\n",
    "DROP_COLS = {\"Date\", \"Store\", \"Customers\", \"LogSales\"}  \n",
    "\n",
    "feature_cols = [c for c in df2.columns if (c not in DROP_COLS) and (c != TARGET)]\n",
    "\n",
    "with open(FEATURE_COLS_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"feature_cols\": feature_cols}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(SPLIT_META_OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(split_meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Scale only FEATURES (Store untouched)\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train = train_df[feature_cols].values\n",
    "X_val   = val_df[feature_cols].values\n",
    "X_test  = test_df[feature_cols].values\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "joblib.dump(scaler, SCALER_OUT)\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Save CSV (scaled features + target + Store + Date)\n",
    "# ----------------------------\n",
    "def to_feature_df(split_df, X_scaled):\n",
    "    out = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "   \n",
    "    out[\"Sales\"] = np.log1p(split_df[\"Sales\"].values) \n",
    "    out[\"Store\"] = split_df[\"Store\"].astype(int).values\n",
    "    out[\"Date\"]  = split_df[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "    return out\n",
    "\n",
    "train_feat = to_feature_df(train_df, X_train_scaled)\n",
    "val_feat   = to_feature_df(val_df,   X_val_scaled)\n",
    "test_feat  = to_feature_df(test_df,  X_test_scaled)\n",
    "\n",
    "train_feat.to_csv(TRAIN_OUT, index=False)\n",
    "val_feat.to_csv(VAL_OUT, index=False)\n",
    "test_feat.to_csv(TEST_OUT, index=False)\n",
    "\n",
    "print(\"Saved CSVs to:\", OUT_DIR)\n",
    "print(\"Shapes:\", train_feat.shape, val_feat.shape, test_feat.shape)\n",
    "print(\"Is 'Customers' leaked into features?\", \"Customers\" in feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68b1d48-c9d9-403e-9bbc-d635f0ff7ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
